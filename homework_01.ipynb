{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_upCOEI3Upu"
   },
   "source": [
    "# Основы глубинного обучения, майнор ИАД\n",
    "\n",
    "## Домашнее задание 1. Введение в PyTorch. Полносвязные нейронные сети.\n",
    "\n",
    "### Общая информация\n",
    "\n",
    "Дата выдачи: 01.10.2023\n",
    "\n",
    "Мягкий дедлайн: 23:59MSK 15.10.2023\n",
    "\n",
    "Жесткий дедлайн: 23:59MSK 20.10.2023\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Максимально допустимая оценка за работу — 10 баллов. За каждый день просрочки снимается 1 балл. Сдавать задание после жёсткого дедлайна сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "Итогова оценка считается как\n",
    "$$\n",
    "min(task_1, task_2)*0.8 + max(task_1, task_2)*0.2\n",
    "$$\n",
    "\n",
    "где task_1 и task_2 - оценки за первое и второе заданиее соответсвенно.\n",
    "Также, за домашнее задание выставляется 0, если не сделано нулевое или третье задание.\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит предсказывать год выпуска песни (**задача регрессии**) по некоторым звуковым признакам: [данные](https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd). В ячейках ниже находится код для загрузки данных. Обратите внимание, что обучающая и тестовая выборки располагаются в одном файле, поэтому НЕ меняйте ячейку, в которой производится деление данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "RI_eoe063VaP",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:47:17.414121Z",
     "start_time": "2024-07-31T16:47:17.351006Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:32.010840Z",
     "start_time": "2024-07-31T16:12:32.003765Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NgSZeU-7vgj",
    "outputId": "bf328cf3-2c32-4f57-fcca-b3801412e2d6",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-30T15:43:18.250235Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget -O data1.txt.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DSVJZzkJ7zZE",
    "outputId": "488c6de3-e897-463b-945e-a6752c113f63",
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:38.577784Z",
     "start_time": "2024-07-31T16:12:32.011835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     0         1         2         3         4         5         6         7   \\\n0  2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905 -25.01202   \n1  2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   8.76630   \n2  2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940  -3.27872   \n3  2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   5.05097   \n4  2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409 -12.48207   \n5  2001  50.54767   0.31568  92.35066  22.38696 -25.51870 -19.04928  20.67345   \n\n         8         9   ...        81         82        83        84        85  \\\n0 -12.23257   7.83089  ...  13.01620  -54.40548  58.99367  15.37344   1.11144   \n1  -0.92019  18.76548  ...   5.66812  -19.68073  33.04964  42.87836  -9.90378   \n2  -2.35035  16.07017  ...   3.03800   26.05866 -50.92779  10.93792  -0.07568   \n3 -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705 -46.67617 -12.51516   \n4  -9.37636  12.63699  ...   9.92661  -55.95724  64.92712 -17.72522  -1.49237   \n5  -5.19943   3.63566  ...   6.59753  -50.69577  26.02574  18.94430  -0.33730   \n\n         86         87        88         89        90  \n0 -23.08793   68.40795  -1.82223  -27.46348   2.26327  \n1 -32.22788   70.49388  12.04941   58.43453  26.92061  \n2  43.20130 -115.00698  -0.05859   39.67068  -0.66345  \n3  82.58061  -72.08993   9.90558  199.62971  18.85382  \n4  -7.50035   51.76631   7.88713   55.66926  28.74903  \n5   6.09352   35.18381   5.00283  -11.02257   0.02263  \n\n[6 rows x 91 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>81</th>\n      <th>82</th>\n      <th>83</th>\n      <th>84</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001</td>\n      <td>49.94357</td>\n      <td>21.47114</td>\n      <td>73.07750</td>\n      <td>8.74861</td>\n      <td>-17.40628</td>\n      <td>-13.09905</td>\n      <td>-25.01202</td>\n      <td>-12.23257</td>\n      <td>7.83089</td>\n      <td>...</td>\n      <td>13.01620</td>\n      <td>-54.40548</td>\n      <td>58.99367</td>\n      <td>15.37344</td>\n      <td>1.11144</td>\n      <td>-23.08793</td>\n      <td>68.40795</td>\n      <td>-1.82223</td>\n      <td>-27.46348</td>\n      <td>2.26327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2001</td>\n      <td>48.73215</td>\n      <td>18.42930</td>\n      <td>70.32679</td>\n      <td>12.94636</td>\n      <td>-10.32437</td>\n      <td>-24.83777</td>\n      <td>8.76630</td>\n      <td>-0.92019</td>\n      <td>18.76548</td>\n      <td>...</td>\n      <td>5.66812</td>\n      <td>-19.68073</td>\n      <td>33.04964</td>\n      <td>42.87836</td>\n      <td>-9.90378</td>\n      <td>-32.22788</td>\n      <td>70.49388</td>\n      <td>12.04941</td>\n      <td>58.43453</td>\n      <td>26.92061</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2001</td>\n      <td>50.95714</td>\n      <td>31.85602</td>\n      <td>55.81851</td>\n      <td>13.41693</td>\n      <td>-6.57898</td>\n      <td>-18.54940</td>\n      <td>-3.27872</td>\n      <td>-2.35035</td>\n      <td>16.07017</td>\n      <td>...</td>\n      <td>3.03800</td>\n      <td>26.05866</td>\n      <td>-50.92779</td>\n      <td>10.93792</td>\n      <td>-0.07568</td>\n      <td>43.20130</td>\n      <td>-115.00698</td>\n      <td>-0.05859</td>\n      <td>39.67068</td>\n      <td>-0.66345</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2001</td>\n      <td>48.24750</td>\n      <td>-1.89837</td>\n      <td>36.29772</td>\n      <td>2.58776</td>\n      <td>0.97170</td>\n      <td>-26.21683</td>\n      <td>5.05097</td>\n      <td>-10.34124</td>\n      <td>3.55005</td>\n      <td>...</td>\n      <td>34.57337</td>\n      <td>-171.70734</td>\n      <td>-16.96705</td>\n      <td>-46.67617</td>\n      <td>-12.51516</td>\n      <td>82.58061</td>\n      <td>-72.08993</td>\n      <td>9.90558</td>\n      <td>199.62971</td>\n      <td>18.85382</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2001</td>\n      <td>50.97020</td>\n      <td>42.20998</td>\n      <td>67.09964</td>\n      <td>8.46791</td>\n      <td>-15.85279</td>\n      <td>-16.81409</td>\n      <td>-12.48207</td>\n      <td>-9.37636</td>\n      <td>12.63699</td>\n      <td>...</td>\n      <td>9.92661</td>\n      <td>-55.95724</td>\n      <td>64.92712</td>\n      <td>-17.72522</td>\n      <td>-1.49237</td>\n      <td>-7.50035</td>\n      <td>51.76631</td>\n      <td>7.88713</td>\n      <td>55.66926</td>\n      <td>28.74903</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2001</td>\n      <td>50.54767</td>\n      <td>0.31568</td>\n      <td>92.35066</td>\n      <td>22.38696</td>\n      <td>-25.51870</td>\n      <td>-19.04928</td>\n      <td>20.67345</td>\n      <td>-5.19943</td>\n      <td>3.63566</td>\n      <td>...</td>\n      <td>6.59753</td>\n      <td>-50.69577</td>\n      <td>26.02574</td>\n      <td>18.94430</td>\n      <td>-0.33730</td>\n      <td>6.09352</td>\n      <td>35.18381</td>\n      <td>5.00283</td>\n      <td>-11.02257</td>\n      <td>0.02263</td>\n    </tr>\n  </tbody>\n</table>\n<p>6 rows × 91 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.txt.zip', header=None)\n",
    "df.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N9a-eJUG35C3"
   },
   "source": [
    "Мы вывели кусок данных, чтобы понять, насколько они пригодны для работы без изменений. Здесь ясно, что сомнительно дальше с такими данными работать, потому что как минимум есть отрицательные значения, которые не отмасштабированы, кроме того еще сразу бросается в глаза совсем разная размерность, где-то видим реально большие числа, а где-то 0.075. Ясно, что будем скейлить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n4wnRJT1778j",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:38.652782Z",
     "start_time": "2024-07-31T16:12:38.580783Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "train_size = 463715\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_386JE_o5gOd"
   },
   "source": [
    "## Задание 0. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)\n",
    "\n",
    "Мы будем использовать RMSE как метрику качества. Для самого первого бейзлайна обучите `Ridge` регрессию из `sklearn`. Кроме того, посчитайте качество при наилучшем константном прогнозе.\n",
    "\n",
    "Для выполнения данного задания (и всех последующих) предобработайте данные.\n",
    "\n",
    "1. Зафиксируйте random_seed везде где только возможно. Вам предоставлена функция для этого, однако вы можете дополнить ее своими дополнениями\n",
    "2. Обучите StandertScaler и предобработайте ваши данные. В следующих заданиях можете использовать другой scaler или вообще отказаться от него\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1kV-kCVs8Aju",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:40.534472Z",
     "start_time": "2024-07-31T16:12:38.653784Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Скалирование\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lkfkXylb8U-O",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:40.539741Z",
     "start_time": "2024-07-31T16:12:40.535466Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GKVVatBw8cH7",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:40.551567Z",
     "start_time": "2024-07-31T16:12:40.540741Z"
    }
   },
   "outputs": [],
   "source": [
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uv7qthu935C4",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:41.169088Z",
     "start_time": "2024-07-31T16:12:40.553565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score=9.552822967970322\n",
      "test_score=9.510160820470436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Ridge model\n",
    "# Не стану подбирать гиперпараметр alpha\n",
    "\n",
    "model = Ridge()\n",
    "rmse_for_model = root_mean_squared_error\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "train_score = rmse_for_model(y_train, model.predict(X_train_scaled))\n",
    "test_score = rmse_for_model(y_test, model.predict(X_test_scaled))\n",
    "print(f\"{train_score=}\")\n",
    "print(f\"{test_score=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HJKGuhFi35C4"
   },
   "source": [
    "Лучшая константа для RMSE это среднее, посчитаем значение метрики при нем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kOcFuy1P35C4",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:41.184769Z",
     "start_time": "2024-07-31T16:12:41.170603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_rmse_metric_train=10.946566221164124\n",
      "best_rmse_metric_test=10.863228020678134\n"
     ]
    }
   ],
   "source": [
    "# Average constant model test\n",
    "\n",
    "average = np.mean(y_train)\n",
    "\n",
    "best_rmse_metric_train = rmse_for_model(y_train, np.full_like(y_train, average))\n",
    "best_rmse_metric_test = rmse_for_model(y_test, np.full_like(y_test, average))\n",
    "print(f\"{best_rmse_metric_train=}\")\n",
    "print(f\"{best_rmse_metric_test=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BDHAnIkS8vNY"
   },
   "source": [
    "## Задание 1. (максимум 10 баллов)\n",
    "\n",
    "Закрепите свои знания о том, как pytorch работает с обратным распространением ошибки, проделав следующие шаги:\n",
    "\n",
    "1. Создайте модель линейной регрессии, которая будет состоять только из одного Linear слоя.\n",
    "2. Напишите цикл обучения вашей линейной регрессии. В нем реализуйте подсчет функции потерь, сделайте шаг градиентного спуска. Запрещено использовать готовые оптимизаторы и loss-функции из библиотеки pytorch. Для подсчета градиента воспользуйтесь методом backward.\n",
    "3. Запустите обучение на 10 эпохах, после каждой проверяйте значение целевой метрики на тестовой выборке.\n",
    "4. Выведите на экран графики метрики и значения функции потерь на тестовой и обучающей выборке.\n",
    "\n",
    "В данном задании нет цели побить какой-то порог по метрике. Ваша задача - убедиться в том, что ваш рукописный цикл обучения работает. Для ускорения вычислений и обучения модели можете брать только срез данных, а не весь датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LxnT6G1J-apf",
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:43.186440Z",
     "start_time": "2024-07-31T16:12:41.187769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "584672633e354362a8de79b137eb461b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train 1997.82666015625, test 1997.8837890625\n",
      "epoch 2, train 1798.041259765625, test 1798.18994140625\n",
      "epoch 3, train 1618.24169921875, test 1618.35986328125\n",
      "epoch 4, train 1456.4232177734375, test 1456.525634765625\n",
      "epoch 5, train 1310.7874755859375, test 1310.8785400390625\n",
      "epoch 6, train 1179.71630859375, test 1179.798828125\n",
      "epoch 7, train 1061.753173828125, test 1061.828857421875\n",
      "epoch 8, train 955.58740234375, test 955.657470703125\n",
      "epoch 9, train 860.0392456054688, test 860.104736328125\n",
      "epoch 10, train 774.0471801757812, test 774.108642578125\n",
      "epoch 11, train 696.6556396484375, test 696.7136840820312\n",
      "epoch 12, train 627.004638671875, test 627.0597534179688\n",
      "epoch 13, train 564.3203125, test 564.3727416992188\n",
      "epoch 14, train 507.9062805175781, test 507.95635986328125\n",
      "epoch 15, train 457.13555908203125, test 457.18353271484375\n",
      "epoch 16, train 411.44415283203125, test 411.49017333984375\n",
      "epoch 17, train 370.3241882324219, test 370.368408203125\n",
      "epoch 18, train 333.31890869140625, test 333.36151123046875\n",
      "epoch 19, train 300.01708984375, test 300.05816650390625\n",
      "epoch 20, train 270.0487976074219, test 270.0884094238281\n",
      "epoch 21, train 243.08094787597656, test 243.11920166015625\n",
      "epoch 22, train 218.81393432617188, test 218.85092163085938\n",
      "epoch 23, train 196.9781036376953, test 197.01385498046875\n",
      "epoch 24, train 177.33079528808594, test 177.3653564453125\n",
      "epoch 25, train 159.65379333496094, test 159.68719482421875\n",
      "epoch 26, train 143.7506103515625, test 143.7828826904297\n",
      "epoch 27, train 129.44448852539062, test 129.47564697265625\n",
      "epoch 28, train 116.57649993896484, test 116.6065673828125\n",
      "epoch 29, train 105.00363159179688, test 105.03260803222656\n",
      "epoch 30, train 94.59732055664062, test 94.62519836425781\n",
      "epoch 31, train 85.24186706542969, test 85.26863861083984\n",
      "epoch 32, train 76.83332061767578, test 76.85897827148438\n",
      "epoch 33, train 69.27806854248047, test 69.30256652832031\n",
      "epoch 34, train 62.492164611816406, test 62.51548385620117\n",
      "epoch 35, train 56.4001350402832, test 56.4222297668457\n",
      "epoch 36, train 50.93402862548828, test 50.95484161376953\n",
      "epoch 37, train 46.032997131347656, test 46.052467346191406\n",
      "epoch 38, train 41.64221954345703, test 41.66027069091797\n",
      "epoch 39, train 37.71263122558594, test 37.729190826416016\n",
      "epoch 40, train 34.20018768310547, test 34.215171813964844\n",
      "epoch 41, train 31.065109252929688, test 31.07840919494629\n",
      "epoch 42, train 28.271820068359375, test 28.283340454101562\n",
      "epoch 43, train 25.788211822509766, test 25.7978458404541\n",
      "epoch 44, train 23.585330963134766, test 23.59296417236328\n",
      "epoch 45, train 21.63694953918457, test 21.64247703552246\n",
      "epoch 46, train 19.919384002685547, test 19.922700881958008\n",
      "epoch 47, train 18.41082763671875, test 18.411842346191406\n",
      "epoch 48, train 17.091291427612305, test 17.08992576599121\n",
      "epoch 49, train 15.942404747009277, test 15.938606262207031\n",
      "epoch 50, train 14.946966171264648, test 14.940705299377441\n",
      "epoch 51, train 14.088921546936035, test 14.080198287963867\n",
      "epoch 52, train 13.353367805480957, test 13.342212677001953\n",
      "epoch 53, train 12.726150512695312, test 12.712627410888672\n",
      "epoch 54, train 12.19421672821045, test 12.178415298461914\n",
      "epoch 55, train 11.745436668395996, test 11.727470397949219\n",
      "epoch 56, train 11.368703842163086, test 11.348706245422363\n",
      "epoch 57, train 11.053933143615723, test 11.032050132751465\n",
      "epoch 58, train 10.79199504852295, test 10.768378257751465\n",
      "epoch 59, train 10.57481861114502, test 10.54962158203125\n",
      "epoch 60, train 10.395329475402832, test 10.368701934814453\n",
      "epoch 61, train 10.24740982055664, test 10.21949291229248\n",
      "epoch 62, train 10.125799179077148, test 10.096729278564453\n",
      "epoch 63, train 10.02599811553955, test 9.995898246765137\n",
      "epoch 64, train 9.944228172302246, test 9.91321086883545\n",
      "epoch 65, train 9.87729263305664, test 9.845459938049316\n",
      "epoch 66, train 9.822538375854492, test 9.78997802734375\n",
      "epoch 67, train 9.777775764465332, test 9.744572639465332\n",
      "epoch 68, train 9.741196632385254, test 9.707417488098145\n",
      "epoch 69, train 9.711285591125488, test 9.676997184753418\n",
      "epoch 70, train 9.68682861328125, test 9.652085304260254\n",
      "epoch 71, train 9.66681957244873, test 9.631668090820312\n",
      "epoch 72, train 9.65042495727539, test 9.614909172058105\n",
      "epoch 73, train 9.636968612670898, test 9.60112476348877\n",
      "epoch 74, train 9.625905990600586, test 9.589768409729004\n",
      "epoch 75, train 9.616805076599121, test 9.580401420593262\n",
      "epoch 76, train 9.609286308288574, test 9.572641372680664\n",
      "epoch 77, train 9.603066444396973, test 9.566202163696289\n",
      "epoch 78, train 9.597895622253418, test 9.560831069946289\n",
      "epoch 79, train 9.593586921691895, test 9.556340217590332\n",
      "epoch 80, train 9.589977264404297, test 9.552563667297363\n",
      "epoch 81, train 9.58693790435791, test 9.549368858337402\n",
      "epoch 82, train 9.584362983703613, test 9.546650886535645\n",
      "epoch 83, train 9.582171440124512, test 9.544327735900879\n",
      "epoch 84, train 9.580293655395508, test 9.542325973510742\n",
      "epoch 85, train 9.578672409057617, test 9.540590286254883\n",
      "epoch 86, train 9.577262878417969, test 9.539073944091797\n",
      "epoch 87, train 9.576031684875488, test 9.537740707397461\n",
      "epoch 88, train 9.574943542480469, test 9.5365571975708\n",
      "epoch 89, train 9.573975563049316, test 9.535499572753906\n",
      "epoch 90, train 9.573107719421387, test 9.5345458984375\n",
      "epoch 91, train 9.572324752807617, test 9.533681869506836\n",
      "epoch 92, train 9.571610450744629, test 9.532890319824219\n",
      "epoch 93, train 9.570956230163574, test 9.532161712646484\n",
      "epoch 94, train 9.570353507995605, test 9.531488418579102\n",
      "epoch 95, train 9.569792747497559, test 9.53085994720459\n",
      "epoch 96, train 9.569270133972168, test 9.530271530151367\n",
      "epoch 97, train 9.568778991699219, test 9.529717445373535\n",
      "epoch 98, train 9.568318367004395, test 9.529194831848145\n",
      "epoch 99, train 9.56788158416748, test 9.528698921203613\n",
      "epoch 100, train 9.567464828491211, test 9.528226852416992\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "epoch = 100  # возьму 100, а не 10, чтобы качественно обучить и сравнить с прошлыми заданиями\n",
    "\n",
    "# init\n",
    "w = torch.rand(X_train.shape[1], requires_grad=True)  # weight\n",
    "b = torch.rand(1, requires_grad=True)  # bias\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)  # data\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)  # target\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)  # data\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)  # target\n",
    "\n",
    "losses_train, losses_test = [], []\n",
    "\n",
    "for e in tqdm(range(epoch)):\n",
    "    # pred\n",
    "    y_pred = torch.matmul(X_train_tensor, w) + b\n",
    "    loss = torch.mean((y_pred - y_train_tensor) ** 2)\n",
    "    loss.backward()\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        losses_train.append(torch.sqrt(loss).item())  # RMSE\n",
    "        y_pred_test = torch.matmul(X_test_tensor, w) + b\n",
    "        losses_test.append(torch.sqrt(torch.mean((y_pred_test - y_test_tensor) ** 2)).item())  # RMSE\n",
    "\n",
    "        # step\n",
    "        w.data -= learning_rate * w.grad\n",
    "        b.data -= learning_rate * b.grad\n",
    "\n",
    "    # zero grad\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    print(f\"epoch {e+1}, train {losses_train[-1]}, test {losses_test[-1]}\")\n",
    "# Результаты почти как в sklearn, не хватило только немного регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "plt.plot(range(1, len(losses_train) + 1), losses_train, label='Training Loss', marker='o', linestyle='-')\n",
    "plt.plot(range(1, len(losses_test) + 1), losses_test, label='Test Loss', color='red', marker='x', linestyle='--')\n",
    "\n",
    "# labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss to Epoch')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0, 1000)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n",
    "\n",
    "# Модель хорошо обучилась. \n",
    "# Тестовая валидация видимо накладывается на лосс тренировочной выборки из за похожести данных между собой и большого количества данных"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:43.422436Z",
     "start_time": "2024-07-31T16:12:43.187437Z"
    }
   },
   "execution_count": 12
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_6ilBKYt6OdD"
   },
   "source": [
    "## Задание 2. (максимум 10 баллов)\n",
    "\n",
    "Реализуйте обучение и тестирование нейронной сети для предоставленного вам набора данных. Соотношение между полученным значением метрики на тестовой выборке и баллами за задание следующее:\n",
    "\n",
    "- $\\text{RMSE} \\le 9.00 $ &mdash; 4 балла\n",
    "- $\\text{RMSE} \\le 8.90 $ &mdash; 6 баллов\n",
    "- $\\text{RMSE} \\le 8.80 $ &mdash; 8 баллов\n",
    "- $\\text{RMSE} \\le 8.75 $ &mdash; 10 баллов\n",
    "\n",
    "Есть несколько правил, которых вам нужно придерживаться:\n",
    "\n",
    "- Весь пайплайн обучения должен быть написан на PyTorch. При этом вы можете пользоваться другими библиотеками (`numpy`, `sklearn` и пр.), но только для обработки данных. То есть как угодно трансформировать данные и считать метрики с помощью этих библиотек можно, а импортировать модели из `sklearn` и выбивать с их помощью требуемое качество &mdash; нельзя. Также нельзя пользоваться библиотеками, для которых сам PyTorch является зависимостью.\n",
    "\n",
    "- Мы никак не ограничиваем ваш выбор архитектуры модели, но скорее всего вам будет достаточно полносвязной нейронной сети.\n",
    "\n",
    "- Для обучения запрещается использовать какие-либо иные данные, кроме обучающей выборки.\n",
    "\n",
    "- Ансамблирование моделей запрещено.\n",
    "\n",
    "### Полезные советы:\n",
    "\n",
    "- Очень вряд ли, что у вас с первого раза получится выбить качество на 10 баллов, поэтому пробуйте разные архитектуры, оптимизаторы и значения гиперпараметров. В идеале при запуске каждого нового эксперимента вы должны менять что-то одно, чтобы точно знать, как этот фактор влияет на качество.\n",
    "\n",
    "- Не забудьте, что для улучшения качества модели вам поможет **нормировка таргета**.\n",
    "\n",
    "- Тот факт, что мы занимаемся глубинным обучением, не означает, что стоит забывать про приемы, использующиеся в классическом машинном обучении. Так что обязательно проводите исследовательский анализ данных, отрисовывайте нужные графики и не забывайте про масштабирование и подбор гиперпараметров.\n",
    "\n",
    "- Вы наверняка столкнетесь с тем, что ваша нейронная сеть будет сильно переобучаться. Для нейросетей существуют специальные методы регуляризации, например, dropout ([статья](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)) и weight decay ([блогпост](https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd)). Они, разумеется, реализованы в PyTorch. Попробуйте поэкспериментировать с ними.\n",
    "\n",
    "- Если вы чего-то не знаете, не гнушайтесь гуглить. В интернете очень много полезной информации, туториалов и советов по глубинному обучению в целом и по PyTorch в частности. Но не забывайте, что за скатанный код без ссылки на источник придется ответить по всей строгости!\n",
    "\n",
    "- Если вы сразу реализуете обучение на GPU, то у вас будет больше времени на эксперименты, так как любые вычисления будут работать быстрее. Google Colab предоставляет несколько GPU-часов (обычно около 8-10) в сутки бесплатно.\n",
    "\n",
    "- Чтобы отладить код, можете обучаться на небольшой части данных или даже на одном батче. Если лосс на обучающей выборке не падает, то что-то точно идет не так!\n",
    "\n",
    "- Пользуйтесь утилитами, которые вам предоставляет PyTorch (например, Dataset и Dataloader). Их специально разработали для упрощения разработки пайплайна обучения.\n",
    "\n",
    "- Скорее всего вы захотите отслеживать прогресс обучения. Для создания прогресс-баров есть удобная библиотека `tqdm`.\n",
    "\n",
    "- Быть может, вы захотите, чтобы графики рисовались прямо во время обучения. Можете воспользоваться функцией [clear_output](http://ipython.org/ipython-doc/dev/api/generated/IPython.display.html#IPython.display.clear_output), чтобы удалять старый график и рисовать новый на его месте.\n",
    "\n",
    "**ОБЯЗАТЕЛЬНО** рисуйте графики зависимости лосса/метрики на обучающей и тестовой выборках в зависимости от времени обучения. Если обучение занимает относительно небольшое число эпох, то лучше рисовать зависимость от номера шага обучения, если же эпох больше, то рисуйте зависимость по эпохам. Если проверяющий не увидит такого графика для вашей лучшей модели, то он в праве снизить баллы за задание.\n",
    "\n",
    "**ВАЖНО!** Ваше решение должно быть воспроизводимым. Если это не так, то проверяющий имеет право снизить баллы за задание. Чтобы зафиксировать random seed, воспользуйтесь функцией из предыдущего задания.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TZW0gMe3vT8u"
   },
   "source": [
    "Вы можете придерживаться любой адекватной струкуры кода, но мы советуем воспользоваться сигнатурами функций, которые приведены ниже. Лучше всего, если вы проверите ваши предсказания ассертом: так вы убережете себя от разных косяков, например, что вектор предсказаний состоит из всего одного числа. В любом случае, внимательно следите за тем, для каких тензоров вы считаете метрику RMSE. При случайном или намеренном введении в заблуждение проверяющие очень сильно разозлятся."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Скалирование\n",
    "scaler = StandardScaler()  # для ReLU лучше использовать MinMaxScaler, чтобы не было нулей\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Думаю для нейронных сетей лучше будет отскалировать таргет (главное потом как-то вернуться к начальным значениям) \n",
    "# scaler_y = MinMaxScaler()  \n",
    "# y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "# y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:47:32.859433Z",
     "start_time": "2024-07-31T16:47:30.183489Z"
    }
   },
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:47:34.803641Z",
     "start_time": "2024-07-31T16:47:34.793507Z"
    }
   },
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T16:20:25.664215Z",
     "start_time": "2024-07-31T16:20:25.619283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = Dataset(X_train_scaled,y_train)# y_train_scaled\n",
    "\n",
    "test_dataset = Dataset(X_test_scaled,y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, num_workers=4, persistent_workers=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_parameters_amount(model: torch.nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:57.424660Z",
     "start_time": "2024-07-31T16:12:57.420572Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создам функции для обучения модели\n",
    "Не уверен что можно использовать PyTorch Lightning, поэтому напишу первый вариант на стандартном pytorch \n",
    "а следующие эксперименты буду уже проводить с PyTorch Lightning для удобства и практики.\n",
    "Возьму за пример обучение на стандартном pytorch без lightning'а из тритьего семинара"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_history(train_history, val_history, title=\"loss\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    steps = np.linspace(0, len(train_history) - 1, len(val_history)).astype(int)\n",
    "\n",
    "    plt.plot(train_history, label='Train')\n",
    "    plt.scatter(steps, val_history, marker=\"+\", s=180, c=\"orange\", label=\"Val\", zorder=2)\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(f\"Training and Validation {title}\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:58.379478Z",
     "start_time": "2024-07-31T16:12:58.374575Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(model, n_epochs, optimizer, criterion, train_loader, test_loader, device=device):\n",
    "    \"\"\"\n",
    "    fit\n",
    "    params:\n",
    "        model - torch.nn.Module to be fitted\n",
    "        optimizer - model optimizer\n",
    "        criterion - loss function from torch.nn\n",
    "        train_loader - torch.utils.data.Dataloader with train set\n",
    "        test_loader - torch.utils.data.Dataloader with test set\n",
    "                      (if you wish to validate during training)\n",
    "    \"\"\"\n",
    "    train_loss_log, val_loss_log = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # тренировка\n",
    "        model.train()\n",
    "        for x, target in tqdm(\n",
    "                train_loader, desc=f\"Training, epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            y_pred = model(x).squeeze()\n",
    "            loss = criterion(y_pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # валидация\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_train = model(train_loader.dataset.x.to(device))\n",
    "            loss_train = torch.sqrt(torch.nn.functional.mse_loss(pred_train, train_loader.dataset.y.to(device)))\n",
    "            \n",
    "            pred_val = model(test_loader.dataset.x.to(device))\n",
    "            loss_val = torch.sqrt(torch.nn.functional.mse_loss(pred_val, test_loader.dataset.y.to(device)))\n",
    "            \n",
    "            train_loss_log.append(loss_train)\n",
    "            val_loss_log.append(loss_val)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            plot_history(train_loss_log, val_loss_log, \"loss\")\n",
    "    \n",
    "            print(\"Train loss:\", loss_train)\n",
    "            print(\"Val loss:\", loss_val)\n",
    "\n",
    "\n",
    "def test(model, criterion, test_loader) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    inference\n",
    "    params:\n",
    "        model - torch.nn.Module to be evaluated on test set\n",
    "        criterion - loss function from torch.nn\n",
    "        test_loader - torch.utils.data.Dataloader with test set\n",
    "    ----------\n",
    "    returns:\n",
    "        predicts - torch.tensor with shape (len(test_loader.dataset), ),\n",
    "                   which contains predictions for test objects\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicts.append(outputs)\n",
    "\n",
    "    predicts = torch.cat(predicts, dim=0)\n",
    "\n",
    "    return predicts\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:58.683206Z",
     "start_time": "2024-07-31T16:12:58.673204Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Тестирование первой модели с двумя скрытыми слоями 128 и 64\n",
    "Также в моделе используется relu для нелинейности, Dropout для регуляризации и batch norm для более плавного обучения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(in_features=90, out_features=128)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(128)\n",
    "        self.linear2 = torch.nn.Linear(in_features=128, out_features=64)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(64)\n",
    "        self.output = torch.nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:59.349719Z",
     "start_time": "2024-07-31T16:12:59.342321Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model1 = Net1().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:12:59.916270Z",
     "start_time": "2024-07-31T16:12:59.741678Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:13:00.040788Z",
     "start_time": "2024-07-31T16:13:00.036507Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "20353"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Количество параметров первой модели\n",
    "get_parameters_amount(model1)  # малюсенькая модель на 20к парметров"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:13:00.258291Z",
     "start_time": "2024-07-31T16:13:00.253744Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model1.parameters(), lr=LR)  # Adam добавляет инерцию и масштабирование градиента\n",
    "criterion = torch.nn.MSELoss()  # RMSE, чтобы можно было сравнить с предыдущими заданиями (а, ну и по заданию rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:13:01.818450Z",
     "start_time": "2024-07-31T16:13:00.519603Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Training, epoch 0:   0%|          | 0/1812 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf8f842fa4a3435c9cad8ed75cdfc018"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir Luzin\\AppData\\Local\\Temp\\ipykernel_14548\\886003218.py:31: UserWarning: Using a target size (torch.Size([463715])) that is different to the input size (torch.Size([463715, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_train = torch.sqrt(torch.nn.functional.mse_loss(pred_train, train_loader.dataset.y.to(device)))\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 801.06 GiB. GPU 0 has a total capacity of 8.00 GiB of which 6.93 GiB is free. Of the allocated memory 20.28 MiB is allocated by PyTorch, and 1.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# на трейне значения выше из-за dropout. На тесте он отключается\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Модель с 8-й эпохи начала \u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m train(model1, EPOCHS, optimizer, criterion, train_dataloader, test_dataloader)\n",
      "Cell \u001B[1;32mIn[18], line 31\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, n_epochs, optimizer, criterion, train_loader, test_loader, device)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     30\u001B[0m     pred_train \u001B[38;5;241m=\u001B[39m model(train_loader\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m---> 31\u001B[0m     loss_train \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mmse_loss(pred_train, train_loader\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mto(device)))\n\u001B[0;32m     33\u001B[0m     pred_val \u001B[38;5;241m=\u001B[39m model(test_loader\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[0;32m     34\u001B[0m     loss_val \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mmse_loss(pred_val, test_loader\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mto(device)))\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\torch\\nn\\functional.py:3384\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3381\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m   3383\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbroadcast_tensors(\u001B[38;5;28minput\u001B[39m, target)\n\u001B[1;32m-> 3384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 801.06 GiB. GPU 0 has a total capacity of 8.00 GiB of which 6.93 GiB is free. Of the allocated memory 20.28 MiB is allocated by PyTorch, and 1.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# на трейне значения выше из-за dropout. На тесте он отключается\n",
    "# Модель с 8-й эпохи начала \n",
    "train(model1, EPOCHS, optimizer, criterion, train_dataloader, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:13:13.203823Z",
     "start_time": "2024-07-31T16:13:01.819450Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assert test(model1, criterion, test_dataloader)[0].shape[0] == y_test.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:13:13.205820Z",
     "start_time": "2024-07-31T16:13:13.205820Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pred = test(model1, criterion, test_dataloader)\n",
    "print(f\"{pred=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:13:13.206820Z",
     "start_time": "2024-07-31T16:13:13.206820Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "next( iter(test_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-31T16:13:13.207816Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробую также использовать Pytorch lighting для сравнения. Вроде его всё таки можно использовать"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class SimpleModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.model(x).squeeze()\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, target = train_batch\n",
    "        predictions = self(x)\n",
    "        loss = torch.nn.functional.mse_loss(predictions, target)\n",
    "        rmse = torch.sqrt(loss)\n",
    "        self.log(\"train_rmse\", rmse, prog_bar=True, logger=True)  # rmse\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)  # loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, target = val_batch\n",
    "        predictions = self(x)\n",
    "        loss = torch.nn.functional.mse_loss(predictions, target)\n",
    "        rmse = torch.sqrt(loss)\n",
    "        self.log(\"val_rmse\", rmse, prog_bar=True, logger=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:47:39.047893Z",
     "start_time": "2024-07-31T16:47:39.027275Z"
    }
   },
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "module1 = SimpleModule(model1, learning_rate=LR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:47:39.403594Z",
     "start_time": "2024-07-31T16:47:39.391789Z"
    }
   },
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode\n",
      "--------------------------------------\n",
      "0 | model | Net1 | 20.4 K | eval\n",
      "--------------------------------------\n",
      "20.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.4 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fb53bc22bad46de88fda5b029879782"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.Dataset'>: it's not the same object as __main__.Dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPicklingError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Для запуска \u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\u001B[39;00m\n\u001B[0;32m      3\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)  \u001B[38;5;66;03m# на 100 значениях было 9.7 - забыл сохранить, но качество всё равно плохое\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(module1, train_dataloader, test_dataloader)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:543\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    541\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 543\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[0;32m    544\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[0;32m    545\u001B[0m )\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:579\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    573\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    574\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    575\u001B[0m     ckpt_path,\n\u001B[0;32m    576\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    577\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    578\u001B[0m )\n\u001B[1;32m--> 579\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[0;32m    581\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:986\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 986\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_stage()\n\u001B[0;32m    988\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    989\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    990\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    991\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1028\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1027\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1028\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[0;32m   1029\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1030\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1057\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1054\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1057\u001B[0m val_loop\u001B[38;5;241m.\u001B[39mrun()\n\u001B[0;32m   1059\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1061\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:113\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskip:\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[1;32m--> 113\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_run_start()\n\u001B[0;32m    115\u001B[0m data_fetcher \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:231\u001B[0m, in \u001B[0;36m_EvaluationLoop.reset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    229\u001B[0m combined_loader\u001B[38;5;241m.\u001B[39mlimits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_batches\n\u001B[0;32m    230\u001B[0m data_fetcher\u001B[38;5;241m.\u001B[39msetup(combined_loader)\n\u001B[1;32m--> 231\u001B[0m \u001B[38;5;28miter\u001B[39m(data_fetcher)  \u001B[38;5;66;03m# creates the iterator inside the fetcher\u001B[39;00m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;66;03m# add the previous `fetched` value to properly track `is_last_batch` with no prefetching\u001B[39;00m\n\u001B[0;32m    234\u001B[0m data_fetcher\u001B[38;5;241m.\u001B[39mfetched \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mcurrent\u001B[38;5;241m.\u001B[39mready\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:104\u001B[0m, in \u001B[0;36m_PrefetchDataFetcher.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_PrefetchDataFetcher\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 104\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    106\u001B[0m         \u001B[38;5;66;03m# ignore pre-fetching, it's not necessary\u001B[39;00m\n\u001B[0;32m    107\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:51\u001B[0m, in \u001B[0;36m_DataFetcher.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_DataFetcher\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcombined_loader)\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:351\u001B[0m, in \u001B[0;36mCombinedLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m _SUPPORTED_MODES[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miterator\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    350\u001B[0m iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflattened, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_limits)\n\u001B[1;32m--> 351\u001B[0m \u001B[38;5;28miter\u001B[39m(iterator)\n\u001B[0;32m    352\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;241m=\u001B[39m iterator\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:155\u001B[0m, in \u001B[0;36m_Sequential.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_current_iterator()\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:173\u001B[0m, in \u001B[0;36m_Sequential._load_current_iterator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_current_iterator\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m# Load a single DataLoader, prevents multiple sets of workers from starting unnecessarily\u001B[39;00m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_idx \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterables):\n\u001B[1;32m--> 173\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterators \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterables[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_idx])]\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m         \u001B[38;5;66;03m# No more iterables to step through, return an empty list\u001B[39;00m\n\u001B[0;32m    176\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterators \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpersistent_workers \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_workers \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 435\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_iterator()\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\u001B[38;5;241m.\u001B[39m_reset(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[1;32m--> 388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _MultiProcessingDataLoaderIter(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m   1031\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1032\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[1;32m-> 1038\u001B[0m w\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\multiprocessing\\process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[0;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    120\u001B[0m _cleanup()\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Popen(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\multiprocessing\\context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_context\u001B[38;5;241m.\u001B[39mget_context()\u001B[38;5;241m.\u001B[39mProcess\u001B[38;5;241m.\u001B[39m_Popen(process_obj)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\multiprocessing\\context.py:336\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_win32\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Popen(process_obj)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     94\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(prep_data, to_child)\n\u001B[1;32m---> 95\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(process_obj, to_child)\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     97\u001B[0m     set_spawning_popen(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32mC:\\Programs\\anaconda\\envs\\dl\\Lib\\multiprocessing\\reduction.py:60\u001B[0m, in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdump\u001B[39m(obj, file, protocol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m     ForkingPickler(file, protocol)\u001B[38;5;241m.\u001B[39mdump(obj)\n",
      "\u001B[1;31mPicklingError\u001B[0m: Can't pickle <class '__main__.Dataset'>: it's not the same object as __main__.Dataset"
     ]
    }
   ],
   "source": [
    "# Для запуска \n",
    "#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "trainer = pl.Trainer(max_epochs=10)  # на 100 значениях было 9.7 - забыл сохранить, но качество всё равно плохое\n",
    "trainer.fit(module1, train_dataloader, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:47:41.453108Z",
     "start_time": "2024-07-31T16:47:39.610442Z"
    }
   },
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b79e34f8ff4640eaa648504ed584c1e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
      "     Validate metric           DataLoader 0\r\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
      "        val_loss            10.531536102294922\r\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'val_loss': 10.531536102294922}]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(module1, dataloaders=test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:15:50.435209Z",
     "start_time": "2024-07-31T16:15:49.521150Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь обучу модель побольше"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.batch_norm0 = torch.nn.BatchNorm1d(90)\n",
    "        self.linear1 = torch.nn.Linear(in_features=90, out_features=1024)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.linear2 = torch.nn.Linear(in_features=1024, out_features=512)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(512)\n",
    "        self.linear3 = torch.nn.Linear(in_features=512, out_features=256)\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(256)\n",
    "        self.linear4 = torch.nn.Linear(in_features=256, out_features=128)\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(128)\n",
    "        self.linear5 = torch.nn.Linear(in_features=128, out_features=64)\n",
    "        self.batch_norm5 = torch.nn.BatchNorm1d(64)\n",
    "        self.linear6 = torch.nn.Linear(in_features=64, out_features=32)\n",
    "        self.batch_norm6 = torch.nn.BatchNorm1d(32)\n",
    "        self.linear7 = torch.nn.Linear(in_features=32, out_features=16)\n",
    "        self.batch_norm7 = torch.nn.BatchNorm1d(16)\n",
    "        self.output = torch.nn.Linear(in_features=16, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear5(x)\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear6(x)\n",
    "        x = self.batch_norm6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear7(x)\n",
    "        x = self.batch_norm7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.output(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:15:50.446127Z",
     "start_time": "2024-07-31T16:15:50.437208Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "model2 = Net2().to(device)\n",
    "module2 = SimpleModule(model2, learning_rate=LR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:15:50.464005Z",
     "start_time": "2024-07-31T16:15:50.447126Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | Net2 | 797 K  | train\n",
      "---------------------------------------\n",
      "797 K     Trainable params\n",
      "0         Non-trainable params\n",
      "797 K     Total params\n",
      "3.189     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63025fcedfc0449f8ce3f061beae5d29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6170620f5bb4eeea371e617604da2d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Для запуска \n",
    "#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "trainer = pl.Trainer(max_epochs=50)\n",
    "trainer.fit(module2, train_dataloader, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T16:15:56.615240Z",
     "start_time": "2024-07-31T16:15:50.466006Z"
    }
   },
   "execution_count": 32
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bine9EES6TIn"
   },
   "source": [
    "## Задание 3. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)\n",
    "\n",
    "Напишите небольшой отчет о том, как вы добились полученного качества: какие средства использовали и какие эксперименты проводили. Подробно расскажите об архитектурах и значениях гиперпараметров, а также какие метрики на тесте они показывали. Чтобы отчет был зачтен, необходимо привести хотя бы 3 эксперимента."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
